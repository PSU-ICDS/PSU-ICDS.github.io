{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Roar Collab User Guide Welcome to the Roar Collab User Guide! The Roar Collab User Guide introduces users to the Institute for Computational and Data Science (ICDS) , to the Roar Collab computing cluster, and provides information on the use of Roar Collab. Contents Overview Connecting Submitting Jobs Handling Data Using Software Notation Angle brackets around an <item> denote the need to replace the entire <item> with the string. The angle brackets should no longer be present after the replacement occurs. Square brackets around an [item] denote an optional item. If used, the entire [item] should be replaced with a string, and the square brackets should no longer be present after the replacement occurs. When commands are provided, they are typically provided with a leading prompt character. When the $ prompt character is used, it denotes that the command is to be run within the system prompt. When the > prompt character is used, it denotes that the command is to be run within software application's prompt. Getting Help After reading the Roar Collab User Guide, you may contact the iAsk Center at icds@psu.edu for further assistance. Also, monitor the ICDS Events page for scheduled training sessions and open office hours hosted by ICDS.","title":"Roar Collab User Guide"},{"location":"#roar-collab-user-guide","text":"Welcome to the Roar Collab User Guide! The Roar Collab User Guide introduces users to the Institute for Computational and Data Science (ICDS) , to the Roar Collab computing cluster, and provides information on the use of Roar Collab.","title":"Roar Collab User Guide"},{"location":"#contents","text":"Overview Connecting Submitting Jobs Handling Data Using Software","title":"Contents"},{"location":"#notation","text":"Angle brackets around an <item> denote the need to replace the entire <item> with the string. The angle brackets should no longer be present after the replacement occurs. Square brackets around an [item] denote an optional item. If used, the entire [item] should be replaced with a string, and the square brackets should no longer be present after the replacement occurs. When commands are provided, they are typically provided with a leading prompt character. When the $ prompt character is used, it denotes that the command is to be run within the system prompt. When the > prompt character is used, it denotes that the command is to be run within software application's prompt.","title":"Notation"},{"location":"#getting-help","text":"After reading the Roar Collab User Guide, you may contact the iAsk Center at icds@psu.edu for further assistance. Also, monitor the ICDS Events page for scheduled training sessions and open office hours hosted by ICDS.","title":"Getting Help"},{"location":"01_Overview/","text":"Overview About ICDS The Institute for Computational and Data Sciences (ICDS) is one of seven interdisciplinary research institutes within Penn State's Office of the Senior Vice President for Research. The mission of ICDS is to build capacity to solve problems of scientific and societal importance through cyber-enabled research. ICDS enables and supports the diverse computational and data science research taking place throughout Penn State. Users come from all corners of the university and conduct interdisciplinary research using the high-performance computing (HPC) systems delivered and supported by ICDS. High-Performance Computing Overview High-performance computing (HPC) is the use of powerful computing systems that are capable of performing complex tasks and solving large-scale computational problems at significantly higher speeds and with greater efficiency than conventional computing systems. These tasks often involve processing and analyzing massive datasets, conducting simulations, modeling complex phenomena, and executing advanced algorithms. The increase in computational performance is a result of the aggregation of computing resources and utilizing those resources in concert to perform the computational process. HPC systems consist of many compute nodes that communicate over fast interconnections. Each node contains many high-speed processors and its own memory. Typically, the nodes also are connected to a shared filesystem. The seamless integration of the compute, storage, and networking components at a large scale is the fundamental essence of HPC. HPC plays a critical role in pushing the boundaries of academic research and enabling breakthroughs in science, engineering, and technology across diverse fields of study. Roar Collab User Flow Diagram Roar Collab System Specs Roar Collab (RC) is the flagship computing cluster for Penn State researchers. Designed with collaboration in mind, the RC environment allows for more frequent software updates and hardware upgrades to keep pace with researchers\u2019 changing needs. RC utilizes the Red Hat Enterprise Linux (RHEL) 8 operating system to provide users with access to compute resources, file storage, and software. RC is a heterogeneous computing cluster comprised of different types of compute nodes, each of which can be categorized as a Basic, Standard, High-Memory, GPU, or Interactive node. Node Type (Designation) Core/Memory Configurations Description Basic ( bc ) 24 cores, 126 GB 64 cores, 255 GB Connected via Ethernet Configured to offer about 4 GB of memory per core Best used for single-node tasks Standard ( sc ) 24 cores, 258 GB 48 cores, 380 GB 48 cores, 512 GB Connected both via Infiniband and Ethernet Infiniband connections provide higher bandwidth inter-node communication Configured to offer about 10 GB of memory per core Good for single-node tasks and also multi-node tasks High-Memory ( hc ) 48 cores, 1 TB 56 cores, 1 TB Connected via Ethernet Configured to offer about 25 GB of memory per core Best for memory-intensive tasks GPU ( gc ) 28 cores, 256 GB 28 cores, 512 GB 48 cores, 380 GB Feature GPUs that can be accessed either individually or collectively Both A100 and P100 GPUs are available Interactive ( ic ) 36 cores, 500 GB Feature GPUs that are specifically configured for GPU-accelerated graphics Best for running graphical software that requires GPU-accelerated graphics Slurm's sinfo Command RC is a heterogeneous computing cluster. To see the different node configurations on RC, use the following command: sinfo --Format=features:40,nodelist:20,cpus:10,memory:10 This sinfo command displays not only the core and memory configuration of the nodes, but it also indicates the processor generation associated with each node. Furthermore, while connected to a specific node, the lscpu command provides more detailed information on the specific processor type available on the node. For nodes with GPU(s), the nvidia-smi command displays more detailed information on the GPU(s) available on that node. Slurm's sinfo documentation page provides a detailed description of the function and options of the sinfo command. Best Practices Roar Collab is shared by many users, and a user's operating behavior can inadvertantly impact system functionality for other users. All users must follow a set of best practices which entail limiting activities that may impact the system for other users. Exercise good citizenship to ensure that your activity does not adversely impact the system and the RC research community. Do Not Run Jobs on the Submit Nodes RC has a few login nodes that are shared among all users. Dozens, and sometimes hundreds, of users may be logged on at any one time accessing the file systems. Think of the submit nodes as a prep area, where users may edit and manage files, perform file management, initiate file transfers, submit new jobs, and track existing batch jobs. The submit nodes provide an interface to the system and to the computational resources. The compute nodes are where intensive computations may be performed and where research software may be utilized. All batch jobs and executables, as well as development and debugging sessions, must be run on the compute nodes. To access compute nodes on RC, either submit a batch job or request an interactive session. The Submitting Jobs section of the RC User Guide provides further details on requesting computational resources. A single user running computationally expensive or disk intensive tasks on a submit node negatively impacts performance for other users. Additionally, since the submit nodes are not configured for intensive computations, the computational performance of such processes is poor. Habitually running jobs on the submit nodes can potentially lead to account suspension. Do Not Use Scratch as a Primary Storage Location Scratch serves as a temporary repository for compute output and is explicitly designed for short-term usage. Unlike other storage locations, scratch is not backed up. Files are subject to automatic removal if they are not accessed within a timeframe of 30 days. The Handling Data section of the RC User Guide provides further details on storage options. Make an Effort to Minimize Resource Requests The amount of time jobs are queued grows as the amount of requested resources increases. To minimize the amount of time a job is queued, minimize the amount of resources requested. It is best to run small test cases to verify that the computational workflow runs successfully before scaling up the process to a large dataset. The Submitting Jobs section of the RC User Guide provides further details on requesting computational resources. Remain Cognizant of Storage Quotas All available storage locations on RC have associated quotas. If the usage of a storage location approaches these quotas, software may not functional nominally and produce cryptic error messages. The Handling Data section of the RC User Guide provides further details on checking storage usage relative to the quotas. Policies The policies regarding the use of RC can be found on the ICDS Policies page.","title":"Overview"},{"location":"01_Overview/#overview","text":"","title":"Overview"},{"location":"01_Overview/#about-icds","text":"The Institute for Computational and Data Sciences (ICDS) is one of seven interdisciplinary research institutes within Penn State's Office of the Senior Vice President for Research. The mission of ICDS is to build capacity to solve problems of scientific and societal importance through cyber-enabled research. ICDS enables and supports the diverse computational and data science research taking place throughout Penn State. Users come from all corners of the university and conduct interdisciplinary research using the high-performance computing (HPC) systems delivered and supported by ICDS.","title":"About ICDS"},{"location":"01_Overview/#high-performance-computing-overview","text":"High-performance computing (HPC) is the use of powerful computing systems that are capable of performing complex tasks and solving large-scale computational problems at significantly higher speeds and with greater efficiency than conventional computing systems. These tasks often involve processing and analyzing massive datasets, conducting simulations, modeling complex phenomena, and executing advanced algorithms. The increase in computational performance is a result of the aggregation of computing resources and utilizing those resources in concert to perform the computational process. HPC systems consist of many compute nodes that communicate over fast interconnections. Each node contains many high-speed processors and its own memory. Typically, the nodes also are connected to a shared filesystem. The seamless integration of the compute, storage, and networking components at a large scale is the fundamental essence of HPC. HPC plays a critical role in pushing the boundaries of academic research and enabling breakthroughs in science, engineering, and technology across diverse fields of study.","title":"High-Performance Computing Overview"},{"location":"01_Overview/#roar-collab-user-flow-diagram","text":"","title":"Roar Collab User Flow Diagram"},{"location":"01_Overview/#roar-collab-system-specs","text":"Roar Collab (RC) is the flagship computing cluster for Penn State researchers. Designed with collaboration in mind, the RC environment allows for more frequent software updates and hardware upgrades to keep pace with researchers\u2019 changing needs. RC utilizes the Red Hat Enterprise Linux (RHEL) 8 operating system to provide users with access to compute resources, file storage, and software. RC is a heterogeneous computing cluster comprised of different types of compute nodes, each of which can be categorized as a Basic, Standard, High-Memory, GPU, or Interactive node. Node Type (Designation) Core/Memory Configurations Description Basic ( bc ) 24 cores, 126 GB 64 cores, 255 GB Connected via Ethernet Configured to offer about 4 GB of memory per core Best used for single-node tasks Standard ( sc ) 24 cores, 258 GB 48 cores, 380 GB 48 cores, 512 GB Connected both via Infiniband and Ethernet Infiniband connections provide higher bandwidth inter-node communication Configured to offer about 10 GB of memory per core Good for single-node tasks and also multi-node tasks High-Memory ( hc ) 48 cores, 1 TB 56 cores, 1 TB Connected via Ethernet Configured to offer about 25 GB of memory per core Best for memory-intensive tasks GPU ( gc ) 28 cores, 256 GB 28 cores, 512 GB 48 cores, 380 GB Feature GPUs that can be accessed either individually or collectively Both A100 and P100 GPUs are available Interactive ( ic ) 36 cores, 500 GB Feature GPUs that are specifically configured for GPU-accelerated graphics Best for running graphical software that requires GPU-accelerated graphics","title":"Roar Collab System Specs"},{"location":"01_Overview/#slurms-sinfo-command","text":"RC is a heterogeneous computing cluster. To see the different node configurations on RC, use the following command: sinfo --Format=features:40,nodelist:20,cpus:10,memory:10 This sinfo command displays not only the core and memory configuration of the nodes, but it also indicates the processor generation associated with each node. Furthermore, while connected to a specific node, the lscpu command provides more detailed information on the specific processor type available on the node. For nodes with GPU(s), the nvidia-smi command displays more detailed information on the GPU(s) available on that node. Slurm's sinfo documentation page provides a detailed description of the function and options of the sinfo command.","title":"Slurm's sinfo Command"},{"location":"01_Overview/#best-practices","text":"Roar Collab is shared by many users, and a user's operating behavior can inadvertantly impact system functionality for other users. All users must follow a set of best practices which entail limiting activities that may impact the system for other users. Exercise good citizenship to ensure that your activity does not adversely impact the system and the RC research community. Do Not Run Jobs on the Submit Nodes RC has a few login nodes that are shared among all users. Dozens, and sometimes hundreds, of users may be logged on at any one time accessing the file systems. Think of the submit nodes as a prep area, where users may edit and manage files, perform file management, initiate file transfers, submit new jobs, and track existing batch jobs. The submit nodes provide an interface to the system and to the computational resources. The compute nodes are where intensive computations may be performed and where research software may be utilized. All batch jobs and executables, as well as development and debugging sessions, must be run on the compute nodes. To access compute nodes on RC, either submit a batch job or request an interactive session. The Submitting Jobs section of the RC User Guide provides further details on requesting computational resources. A single user running computationally expensive or disk intensive tasks on a submit node negatively impacts performance for other users. Additionally, since the submit nodes are not configured for intensive computations, the computational performance of such processes is poor. Habitually running jobs on the submit nodes can potentially lead to account suspension. Do Not Use Scratch as a Primary Storage Location Scratch serves as a temporary repository for compute output and is explicitly designed for short-term usage. Unlike other storage locations, scratch is not backed up. Files are subject to automatic removal if they are not accessed within a timeframe of 30 days. The Handling Data section of the RC User Guide provides further details on storage options. Make an Effort to Minimize Resource Requests The amount of time jobs are queued grows as the amount of requested resources increases. To minimize the amount of time a job is queued, minimize the amount of resources requested. It is best to run small test cases to verify that the computational workflow runs successfully before scaling up the process to a large dataset. The Submitting Jobs section of the RC User Guide provides further details on requesting computational resources. Remain Cognizant of Storage Quotas All available storage locations on RC have associated quotas. If the usage of a storage location approaches these quotas, software may not functional nominally and produce cryptic error messages. The Handling Data section of the RC User Guide provides further details on checking storage usage relative to the quotas.","title":"Best Practices"},{"location":"01_Overview/#policies","text":"The policies regarding the use of RC can be found on the ICDS Policies page.","title":"Policies"},{"location":"02_Connecting/","text":"Connecting Accounts Account Activation All individuals who have an active Penn State access account may request access to Roar Collab (RC) by submitting an account request . Non-PSU Affiliates For any external collaborators, a university faculty member must set up a sponsored access account with the university Accounts Office to provide the collaborator with an access account. Once the collaborator's access account is active, submit an account request to RC. Connecting to Roar Collab Users can connect to RC either through the RC Portal ( rcportal.hpc.psu.edu ) or via an ssh connection to the submit.hpc.psu.edu host. Using the Roar Collab Portal Users can connect to RC through the RC Portal powered by Open OnDemand. Open OnDemand is an NSF-funded, open-source HPC portal that provides users with a simple graphical web interface to HPC resources. Users can submit and monitor jobs, manage files, and run applications using just a web browser. The RC Portal offers many familiar graphical development environments including JupyterLab, RStudio, and software-specific GUIs. The Portal features multiple built-in tools which can be accessed via the top menu bar on the Portal: Apps: Lists all available Portal apps Files: Provides a convenient graphical file manager and lists primary accessible file locations Jobs: Lists active jobs and allows use of the Job Composer Clusters: Provides shell access to submit nodes on RC Interactive Apps: Provides access to all the Portal interactive apps and interactive servers User Tools: Provides access to the User Filesystem Quotas display My Interactive Sessions: Lists any active sessions Connecting via ssh Those who prefer to utilize only the command line environment can connect using Secure Shell (SSH). Through the terminal on macOS or Linux or the command prompt on Windows, users can connect using the following command: $ ssh <userid>@submit.hpc.psu.edu To connect, an RC account linked to an active Penn State access account user ID and password is required. By default, port 22 is used for secure shell connections. A password must be entered and then multi-factor authentication must be completed successfully to complete the login. Note The connection to the system is made with a submit node. Submit nodes are configured primarily to handle incoming user connections and non-intensive computational tasks like editing small files. To perform computational tasks, compute resources must be used. See Submitting Jobs for more details. Linux Commands Quick Reference Command Description ls Lists the files in the current working directory cd Changes the current directory in order to navigate to a new directory mv Moves a file or directory to a new location mkdir Makes a directory rmdir Removes an empty directory touch Creates a file rm Removes a file (or a directory using the -r option) locate Locates a file in a directory clear Clears the terminal of all previous outputs history Shows the history of previous commands find Finds files in a directory grep Searches files or outputs awk A programming language for pattern scanning and processing id Shows the list of groups for a user du Shows disk usage env Prints the current environment variables less Displays a file cp Copies a file (or a directory using the -r option) alias Creates an alias, which is essentially an abbreviated command pwd Prints the current working directory chmod Changes file permissions chgrp Changes group for a file or directory ldd Shows the shared libraries required for an executable or library top Displays the node usage /usr/bin/time Shows time and memory statistics for a command being run bg Continues running a paused task in the background fg Brings a background task into the foreground Ctrl + c Kills a process Ctrl + z Suspends a process Ctrl + r Searches the command history for a string Special characters are useful in many commands. Character Description ~ Indicates the home directory . Indicates current working directory .. Indicates parent of current working directory * Wildcard character for any string | Connects the output of a command to the input of another > Redirects a command output For complete details on any command listed above and more, use man <command> in a terminal session to display the manual page for the command or search online for more detailed usage of fundamental Linux commands.","title":"Connecting"},{"location":"02_Connecting/#connecting","text":"","title":"Connecting"},{"location":"02_Connecting/#accounts","text":"","title":"Accounts"},{"location":"02_Connecting/#account-activation","text":"All individuals who have an active Penn State access account may request access to Roar Collab (RC) by submitting an account request .","title":"Account Activation"},{"location":"02_Connecting/#non-psu-affiliates","text":"For any external collaborators, a university faculty member must set up a sponsored access account with the university Accounts Office to provide the collaborator with an access account. Once the collaborator's access account is active, submit an account request to RC.","title":"Non-PSU Affiliates"},{"location":"02_Connecting/#connecting-to-roar-collab","text":"Users can connect to RC either through the RC Portal ( rcportal.hpc.psu.edu ) or via an ssh connection to the submit.hpc.psu.edu host.","title":"Connecting to Roar Collab"},{"location":"02_Connecting/#using-the-roar-collab-portal","text":"Users can connect to RC through the RC Portal powered by Open OnDemand. Open OnDemand is an NSF-funded, open-source HPC portal that provides users with a simple graphical web interface to HPC resources. Users can submit and monitor jobs, manage files, and run applications using just a web browser. The RC Portal offers many familiar graphical development environments including JupyterLab, RStudio, and software-specific GUIs. The Portal features multiple built-in tools which can be accessed via the top menu bar on the Portal: Apps: Lists all available Portal apps Files: Provides a convenient graphical file manager and lists primary accessible file locations Jobs: Lists active jobs and allows use of the Job Composer Clusters: Provides shell access to submit nodes on RC Interactive Apps: Provides access to all the Portal interactive apps and interactive servers User Tools: Provides access to the User Filesystem Quotas display My Interactive Sessions: Lists any active sessions","title":"Using the Roar Collab Portal"},{"location":"02_Connecting/#connecting-via-ssh","text":"Those who prefer to utilize only the command line environment can connect using Secure Shell (SSH). Through the terminal on macOS or Linux or the command prompt on Windows, users can connect using the following command: $ ssh <userid>@submit.hpc.psu.edu To connect, an RC account linked to an active Penn State access account user ID and password is required. By default, port 22 is used for secure shell connections. A password must be entered and then multi-factor authentication must be completed successfully to complete the login. Note The connection to the system is made with a submit node. Submit nodes are configured primarily to handle incoming user connections and non-intensive computational tasks like editing small files. To perform computational tasks, compute resources must be used. See Submitting Jobs for more details.","title":"Connecting via ssh"},{"location":"02_Connecting/#linux-commands-quick-reference","text":"Command Description ls Lists the files in the current working directory cd Changes the current directory in order to navigate to a new directory mv Moves a file or directory to a new location mkdir Makes a directory rmdir Removes an empty directory touch Creates a file rm Removes a file (or a directory using the -r option) locate Locates a file in a directory clear Clears the terminal of all previous outputs history Shows the history of previous commands find Finds files in a directory grep Searches files or outputs awk A programming language for pattern scanning and processing id Shows the list of groups for a user du Shows disk usage env Prints the current environment variables less Displays a file cp Copies a file (or a directory using the -r option) alias Creates an alias, which is essentially an abbreviated command pwd Prints the current working directory chmod Changes file permissions chgrp Changes group for a file or directory ldd Shows the shared libraries required for an executable or library top Displays the node usage /usr/bin/time Shows time and memory statistics for a command being run bg Continues running a paused task in the background fg Brings a background task into the foreground Ctrl + c Kills a process Ctrl + z Suspends a process Ctrl + r Searches the command history for a string Special characters are useful in many commands. Character Description ~ Indicates the home directory . Indicates current working directory .. Indicates parent of current working directory * Wildcard character for any string | Connects the output of a command to the input of another > Redirects a command output For complete details on any command listed above and more, use man <command> in a terminal session to display the manual page for the command or search online for more detailed usage of fundamental Linux commands.","title":"Linux Commands Quick Reference"},{"location":"03_SubmittingJobs/","text":"Submitting Jobs Running Jobs with Slurm The Roar Collab (RC) computing cluster is a shared computational resource. To perform computationally-intensive tasks, users must request compute resources and be provided access to those resources. The request/provision process allows the tasks of many users to be scheduled and carried out efficiently to avoid resource contention. Slurm (Simple Linux Utility for Resource Management) is utilized by RC as the job scheduler and resource manager. Slurm is an open source, fault-tolerant, and highly scalable cluster management and job scheduling system for Linux clusters, and Slurm is rapidly rising in popularity and many other HPC systems use Slurm as well. Its primary functions are to Allocate access to compute resources to users for some duration of time Provide a framework for starting, executing, and monitoring work on the set of allocated compute resources Arbitrate contention for resources by managing a queue of pending work Warning Do not perform computationally intensive tasks on submit nodes. Submit a resource request via Slurm for computational resources so your computational task can be performed on a compute node. Slurm Resource Directives A compute session can reached via either a batch job or an interactive job. The following sections provide more details on intiating compute sessions: Batch Jobs , Interactive Jobs , and Interactive Jobs Through the RC Portal Resource directives are used to request specific compute resources for a compute session. Resource Directive Description -J or --job-name Specify a name for the job -A or --account Charge resources used by a job to specified account -p or --partition Request a partition for the resource allocation -N or --nodes Request a number of nodes -n or --ntasks Request a number of tasks --ntasks-per-node Request a number of tasks per allocated node --mem Specify the amount of memory required per node --mem-per-cpu Specify the amount of memory required per CPU -t or --time Set a limit on the total run time -C or --constraint Specify any required node features -e or --error Connect script's standard error to a non-default file -o or --output Connect script's standard output to a non-default file --requeue Specify that the batch job should be eligible for requeuing --exclusive Require exclusive use of nodes reserved for job Both standard output and standard error are directed to the same file by default, and the file name is slurm-%j.out , where the %j is replaced by the job ID. The output and error filenames are customizable, however, using the table of symbols below. Symbol Description %j Job ID %x Job name %u Username %N Hostname where the job is running %A Job array's master job allocation number %a Job array ID (index) number Slurm makes use of environment variables within the scope of a job, and utilizing these variables can be beneficial in many cases. Environment Variable Description SLURM_JOB_ID ID of the job SLURM_JOB_NAME Name of job SLURM_NNODES Number of nodes SLURM_NODELIST List of nodes SLURM_NTASKS Total number of tasks SLURM_NTASKS_PER_NODE Number of tasks per node SLURM_QUEUE Queue (partition) SLURM_SUBMIT_DIR Directory of job submission Further details on the available resource directives for Slurm are defined by Slurm in the documentation of the salloc and sbatch commands. A Note on Requesting Resources The resource directives should be populated with resource requests that are adequate to complete the job but should be minimal enough that the job can be placed somewhat quickly by the scheduler. The total time to completion of a job consists of the amount of time the job is queued plus the amount of time it takes the job to run to completion once placed. The queue time is minimized when the bare minimum amount of resources are requested, and the queue time grows as the amount of requested resources grows. The run time of the job is minimized when all of the computational resources available to the job are efficiently utilized. The total time to completion, therefore, is minimized when the resources requested closely match the amount of computational resources that can be efficiently utilized by the job. During the development of the computational job, it is best to keep track of an estimate of the computational resources used by the job. Add about a 20% margin on top of the best estimate of the job's resource usage in order to produce a job's resource requests used in the scheduler directives. It's useful to examine the amount of resources that a single laptop computer has, or 1 laptop-worth of resources , as a reference. A modern above-average laptop, for example, may have an 8-core processor and 32 GB of RAM. If a computational task can run on a laptop without crashing the device, then there is absolutely no need to submit a resource request larger than this. Interactive Jobs The submit nodes of RC are designed to handle very simple tasks such as connections, file editing, and submitting jobs. Performing intensive computations on submit nodes will not only be computationally inefficient, but it will also adversely impact other users' ability to interact with the system. For this reason, users that want to perform computations interactively should do so on compute nodes using the salloc command. To work interactively on a compute node with a single processor core for one hour, use the following command: $ salloc --nodes=1 --ntasks=1 --mem=1G --time=01:00:00 The above command submits a request to the scheduler to queue an interactive job, and when the scheduler is able to place the request, the prompt will return. The hostname in the prompt will change from the previous submit node name to a compute node. Now on a compute node, intensive computational tasks can be performed interactively. This session will be terminated either when the time limit is reached or when the exit command is entered. After the interactive session completes, the session will return to the previous submit node. Interactive Jobs Through the Roar Collab Portal The RC Portal is a simple graphical web interface that provides users with access to RC. Users can submit and monitor jobs, manage files, and run applications using just a web browser. To access the RC Portal, users must log in using valid Penn State access account credentials and must also have an account on RC. The RC Portal is available at the following webpage: https://rcportal.hpc.psu.edu Batch Jobs On RC, users can run jobs by submitting scripts to the Slurm job scheduler. A Slurm script must do three things: Prescribe the resource requirements for the job Set the job's environment Specify the work to be carried out in the form of shell commands The portion of the job that prescribes the resource requirements contains the resource directives. Resource directives in Slurm submission scripts are denoted by lines starting with the #SBATCH keyword. The rest of the script, which both sets the environment and specifies the work to be done, consists of bash commands. The very first line of the submission script, #!/bin/bash , is called a shebang and specifies to the command line environment to interpret the commands as bash commands. Below is a sample Slurm script for running a Python task: #!/bin/bash #SBATCH --job-name=apythonjob # give the job a name #SBATCH --account=open # specify the account #SBATCH --partition=open # specify the partition #SBATCH --nodes=1 # request a node #SBATCH --ntasks=1 # request a task / cpu #SBATCH --mem=1G # request the memory required per node #SBATCH --time=00:01:00 # set a limit on the total run time python pyscript.py In this sample submission script, the resource directives request a single node with a single task . Slurm is a task-based scheduler, and a task is equivalent to a processor core unless otherwise specified in the submission script. The scheduler directives then request 1 GB of memory per node for a maximum of 1 minute of runtime. The memory can be specified in KB, MB, GB, or TB by using a suffix of K, M, G, or T, respectively. If no suffix is used, the default is MB. Lastly, the work to be done is specified, which is the execution of a Python script in this case. If the above sample submission script was saved as pyjob.slurm , it would be submitted to the Slurm scheduler with the sbatch command. $ sbatch pyjob.slurm The job can be submitted to the scheduler from any node on RC. The scheduler will keep the job in the job queue until the job gains sufficient priority to run on a compute node. Depending on the nature of the job and the availability of computational resources, the queue time will vary between seconds to days. To check the status of queued and running jobs, use the squeue command: $ squeue -u <userid> Using Dedicated Partitions Open Queue All RC users have access to the open queue, which allows users to submit jobs free of charge. The open queue has a maximum time request limit of 48 hours and also has several other resource limits per user. The per-user resource limits for the open queue can be displayed with the following command: $ sacctmgr show qos open format=name%10,maxtrespu%40 Jobs on the open queue will start and run only when sufficient idle compute resources are available. For this reason, there is no guarantee on when an open queue job will start. All users have equal priority on the open queue, but open queue jobs have a lower priority than jobs submitted to a paid compute allocation. If compute resources are required for higher priority jobs, then an open queue job may be cancelled so that the higher priority job can be placed. The cancellation of a running job to free resources for a higher priority job is called preemption. By using the --requeue option in a submission script, a job will re-enter the job queue automatically in the event that it is preempted. Furthermore, it is highly recommended for users to break down any large computational workflows into smaller, more manageable computational units so jobs can save state throughout the stages of the workflow. Saving state at set checkpoints will allow the computational workflow to return to the latest checkpoint, reducing the amount of required re- computation in the case that a job is interrupted for any reason. RC has somewhat low utilization, however, so the vast majority of open queue jobs can be submitted and placed in a resonable amount of time. The open queue is entirely adequate for most individual users and for many use cases. Compute Allocations A paid compute allocation provides access to specific compute resources for an individual user or for a group of users. A paid compute allocation provides the following benefits: Guaranteed job start time within one hour No job preemption for non-burst jobs Burst capability up to 4x of the allocation's compute resources A compute allocation results in the creation of a compute account on RC. The mybalance command on RC lists accessible compute accounts and resource information associated with those compute accounts. Use the mybalance -h option for additional command usage information. To submit a job to a paid compute account, supply the -A or --account resource directive with the compute account name and supply the -p or --partition resource directive with sla-prio : #SBATCH -A <compute_account> #SBATCH -p sla-prio To enable bursting, if enabled for the compute account, supply the -p or --partition resource directive with the desired level of bursting for the job ( burst2x , burst3x , burst4x , and so on). To list the available compute accounts and the associated available burst partitions, use the following command: $ sacctmgr show User $(whoami) --associations format=account%30,qos%40 Compute Resources Available A paid compute allocation will typically cover a certain number of cores across a certain timeframe. The resources associated with a compute allocation are in units of core-hours . The compute allocation has an associated Limit in core-hours based on the initial compute allocation agreement. Any amount of compute resources used on the compute allocation results in an accrual of the compute allocation's Usage , again in core-hours . The compute allocation's Balance is simply the Limit minus its Usage . Balance [core-hours] = Limit [core-hours] - Usage [core-hours] At the start of the compute allocation, 60 days-worth of compute resources are added to the compute allocation's Limit . Each day thereafter, 1 day-worth of compute resources are added to the Limit . Initial Resources [core-hours] = # cores * 24 hours/day * 60 days Daily Replenishment [core-hours] = # cores * 24 hours/day The daily replenishment scheme continues on schedule for the life of the compute allocation. Near the very end of the compute allocation, the replenishment schedule may be impacted by the enforced limit on the maximum allowable Balance . The Balance for a compute allocation cannot exceed the amount of compute resources for a window of 91 days and cannot exceed the amount usable by a 4x burst for the remaining life of the compute allocation. This limit is only relevant for the replenishment schedule nearing the very end of the compute allocation life. Max Allowable Balance [core-hours] = min( WindowMaxBalance, 4xBurstMaxBalance ) where WindowMaxBalance [core-hours] = # cores * 24 hours/day * 91 days 4xBurstMaxBalance [core-hours] = # cores * 24 hours/day * # days remaining * 4 burst factor Roar Collab Compute Account Self-Management Modifying Allocation Coordinators The principal contact for a compute allocation is automatically designated as a coordinator for the compute account associated with the compute allocation. A coordinator can add or remove another coordinator with the following command: $ sacctmgr add coordinator account=<compute-account> name=<userid> $ sacctmgr remove coordinator account=<compute-account> name=<userid> Adding and Removing Users from an Allocation A coordinator can then add and remove users from the compute account using the following: $ sacctmgr add user account=<compute-account> name=<userid> $ sacctmgr remove user account=<compute-account> name=<userid> Using GPUs GPUs are available on RC to users that are added to paid GPU compute accounts. To use GPUs, add the --gpus resource directive: #!/bin/bash #SBATCH --job-name=apythonjob # give the job a name #SBATCH --account=<gpu_acct> # specify the account #SBATCH --partition=sla-prio # specify the partition #SBATCH --nodes=1 # request a node #SBATCH --ntasks=1 # request a task / cpu #SBATCH --mem=1G # request the memory required per node #SBATCH --gpus=1 # request a gpu #SBATCH --time=00:01:00 # set a limit on the total run time python pyscript.py Requesting GPU resources for a job is only beneficial if the software running within the job is GPU-enabled. Job Management and Monitoring A user can find the job ID, the assigned node(s), and other useful information using the squeue command. Specifically, the following command displays all running and queued jobs for a specific user: $ squeue -u <user> A useful environment variable is the SQUEUE_FORMAT variable which enables customization of the details shown by the squeue command. This variable can be set, for example, with the following command to provide a highly descriptive squeue output: $ export SQUEUE_FORMAT=\"%.9i %9P %35j %.8u %.2t %.12M %.12L %.5C %.7m %.4D %R\" Further details on the usage of this variable are available on Slurm's squeue documentation page. Another useful job monitoring command is: $ scontrol show job <jobid> Also, a job can be cancelled with $ scancel <jobid> Valuable information can be obtained by monitoring a job on the compute node(s) as the job runs. Connect to the compute node of a running job with the ssh command. Note that a compute node can only be reached if the user has a resource reservation on that specific node. After connecting to the compute node, the top and ps commands are useful tools. $ ssh <comp-node-id> $ top -Hu <user> $ ps -aux | grep <user> Converting from PBS to Slurm Slurm's commands and scheduler directives can be mapped to and from PBS/Torque commands and scheduler directives. To convert any PBS/Torque scripts and/or workflows to Slurm, the commands and scheduler directives should be swapped out and reconfigured. See the table below for the mapping of some common commands and scheduler directives: Action PBS/Torque Command Slurm Command Submit a batch job qsub sbatch Request an interactive job qsub -I salloc Cancel a job qdel scancel Check job status qstat squeue Check job status for specific user qstat -u <user> squeue -u <user> Hold a job qhold scontrol hold Release a job qrls scontrol release Resource Request PBS/Torque Directive Slurm Directive Directive designator #PBS #SBATCH Number of nodes -l nodes -N or --nodes Number of CPUs -l ppn -n or --ntasks Amount of memory -l mem --mem or --mem-per-cpu Walltime -l walltime -t or --time Compute account -A -A or --account For a more complete list of command, scheduler directive, and option comparisons, see the Slurm Rosetta Stone .","title":"Submitting Jobs"},{"location":"03_SubmittingJobs/#submitting-jobs","text":"","title":"Submitting Jobs"},{"location":"03_SubmittingJobs/#running-jobs-with-slurm","text":"The Roar Collab (RC) computing cluster is a shared computational resource. To perform computationally-intensive tasks, users must request compute resources and be provided access to those resources. The request/provision process allows the tasks of many users to be scheduled and carried out efficiently to avoid resource contention. Slurm (Simple Linux Utility for Resource Management) is utilized by RC as the job scheduler and resource manager. Slurm is an open source, fault-tolerant, and highly scalable cluster management and job scheduling system for Linux clusters, and Slurm is rapidly rising in popularity and many other HPC systems use Slurm as well. Its primary functions are to Allocate access to compute resources to users for some duration of time Provide a framework for starting, executing, and monitoring work on the set of allocated compute resources Arbitrate contention for resources by managing a queue of pending work Warning Do not perform computationally intensive tasks on submit nodes. Submit a resource request via Slurm for computational resources so your computational task can be performed on a compute node.","title":"Running Jobs with Slurm"},{"location":"03_SubmittingJobs/#slurm-resource-directives","text":"A compute session can reached via either a batch job or an interactive job. The following sections provide more details on intiating compute sessions: Batch Jobs , Interactive Jobs , and Interactive Jobs Through the RC Portal Resource directives are used to request specific compute resources for a compute session. Resource Directive Description -J or --job-name Specify a name for the job -A or --account Charge resources used by a job to specified account -p or --partition Request a partition for the resource allocation -N or --nodes Request a number of nodes -n or --ntasks Request a number of tasks --ntasks-per-node Request a number of tasks per allocated node --mem Specify the amount of memory required per node --mem-per-cpu Specify the amount of memory required per CPU -t or --time Set a limit on the total run time -C or --constraint Specify any required node features -e or --error Connect script's standard error to a non-default file -o or --output Connect script's standard output to a non-default file --requeue Specify that the batch job should be eligible for requeuing --exclusive Require exclusive use of nodes reserved for job Both standard output and standard error are directed to the same file by default, and the file name is slurm-%j.out , where the %j is replaced by the job ID. The output and error filenames are customizable, however, using the table of symbols below. Symbol Description %j Job ID %x Job name %u Username %N Hostname where the job is running %A Job array's master job allocation number %a Job array ID (index) number Slurm makes use of environment variables within the scope of a job, and utilizing these variables can be beneficial in many cases. Environment Variable Description SLURM_JOB_ID ID of the job SLURM_JOB_NAME Name of job SLURM_NNODES Number of nodes SLURM_NODELIST List of nodes SLURM_NTASKS Total number of tasks SLURM_NTASKS_PER_NODE Number of tasks per node SLURM_QUEUE Queue (partition) SLURM_SUBMIT_DIR Directory of job submission Further details on the available resource directives for Slurm are defined by Slurm in the documentation of the salloc and sbatch commands.","title":"Slurm Resource Directives"},{"location":"03_SubmittingJobs/#a-note-on-requesting-resources","text":"The resource directives should be populated with resource requests that are adequate to complete the job but should be minimal enough that the job can be placed somewhat quickly by the scheduler. The total time to completion of a job consists of the amount of time the job is queued plus the amount of time it takes the job to run to completion once placed. The queue time is minimized when the bare minimum amount of resources are requested, and the queue time grows as the amount of requested resources grows. The run time of the job is minimized when all of the computational resources available to the job are efficiently utilized. The total time to completion, therefore, is minimized when the resources requested closely match the amount of computational resources that can be efficiently utilized by the job. During the development of the computational job, it is best to keep track of an estimate of the computational resources used by the job. Add about a 20% margin on top of the best estimate of the job's resource usage in order to produce a job's resource requests used in the scheduler directives. It's useful to examine the amount of resources that a single laptop computer has, or 1 laptop-worth of resources , as a reference. A modern above-average laptop, for example, may have an 8-core processor and 32 GB of RAM. If a computational task can run on a laptop without crashing the device, then there is absolutely no need to submit a resource request larger than this.","title":"A Note on Requesting Resources"},{"location":"03_SubmittingJobs/#interactive-jobs","text":"The submit nodes of RC are designed to handle very simple tasks such as connections, file editing, and submitting jobs. Performing intensive computations on submit nodes will not only be computationally inefficient, but it will also adversely impact other users' ability to interact with the system. For this reason, users that want to perform computations interactively should do so on compute nodes using the salloc command. To work interactively on a compute node with a single processor core for one hour, use the following command: $ salloc --nodes=1 --ntasks=1 --mem=1G --time=01:00:00 The above command submits a request to the scheduler to queue an interactive job, and when the scheduler is able to place the request, the prompt will return. The hostname in the prompt will change from the previous submit node name to a compute node. Now on a compute node, intensive computational tasks can be performed interactively. This session will be terminated either when the time limit is reached or when the exit command is entered. After the interactive session completes, the session will return to the previous submit node.","title":"Interactive Jobs"},{"location":"03_SubmittingJobs/#interactive-jobs-through-the-roar-collab-portal","text":"The RC Portal is a simple graphical web interface that provides users with access to RC. Users can submit and monitor jobs, manage files, and run applications using just a web browser. To access the RC Portal, users must log in using valid Penn State access account credentials and must also have an account on RC. The RC Portal is available at the following webpage: https://rcportal.hpc.psu.edu","title":"Interactive Jobs Through the Roar Collab Portal"},{"location":"03_SubmittingJobs/#batch-jobs","text":"On RC, users can run jobs by submitting scripts to the Slurm job scheduler. A Slurm script must do three things: Prescribe the resource requirements for the job Set the job's environment Specify the work to be carried out in the form of shell commands The portion of the job that prescribes the resource requirements contains the resource directives. Resource directives in Slurm submission scripts are denoted by lines starting with the #SBATCH keyword. The rest of the script, which both sets the environment and specifies the work to be done, consists of bash commands. The very first line of the submission script, #!/bin/bash , is called a shebang and specifies to the command line environment to interpret the commands as bash commands. Below is a sample Slurm script for running a Python task: #!/bin/bash #SBATCH --job-name=apythonjob # give the job a name #SBATCH --account=open # specify the account #SBATCH --partition=open # specify the partition #SBATCH --nodes=1 # request a node #SBATCH --ntasks=1 # request a task / cpu #SBATCH --mem=1G # request the memory required per node #SBATCH --time=00:01:00 # set a limit on the total run time python pyscript.py In this sample submission script, the resource directives request a single node with a single task . Slurm is a task-based scheduler, and a task is equivalent to a processor core unless otherwise specified in the submission script. The scheduler directives then request 1 GB of memory per node for a maximum of 1 minute of runtime. The memory can be specified in KB, MB, GB, or TB by using a suffix of K, M, G, or T, respectively. If no suffix is used, the default is MB. Lastly, the work to be done is specified, which is the execution of a Python script in this case. If the above sample submission script was saved as pyjob.slurm , it would be submitted to the Slurm scheduler with the sbatch command. $ sbatch pyjob.slurm The job can be submitted to the scheduler from any node on RC. The scheduler will keep the job in the job queue until the job gains sufficient priority to run on a compute node. Depending on the nature of the job and the availability of computational resources, the queue time will vary between seconds to days. To check the status of queued and running jobs, use the squeue command: $ squeue -u <userid>","title":"Batch Jobs"},{"location":"03_SubmittingJobs/#using-dedicated-partitions","text":"","title":"Using Dedicated Partitions"},{"location":"03_SubmittingJobs/#open-queue","text":"All RC users have access to the open queue, which allows users to submit jobs free of charge. The open queue has a maximum time request limit of 48 hours and also has several other resource limits per user. The per-user resource limits for the open queue can be displayed with the following command: $ sacctmgr show qos open format=name%10,maxtrespu%40 Jobs on the open queue will start and run only when sufficient idle compute resources are available. For this reason, there is no guarantee on when an open queue job will start. All users have equal priority on the open queue, but open queue jobs have a lower priority than jobs submitted to a paid compute allocation. If compute resources are required for higher priority jobs, then an open queue job may be cancelled so that the higher priority job can be placed. The cancellation of a running job to free resources for a higher priority job is called preemption. By using the --requeue option in a submission script, a job will re-enter the job queue automatically in the event that it is preempted. Furthermore, it is highly recommended for users to break down any large computational workflows into smaller, more manageable computational units so jobs can save state throughout the stages of the workflow. Saving state at set checkpoints will allow the computational workflow to return to the latest checkpoint, reducing the amount of required re- computation in the case that a job is interrupted for any reason. RC has somewhat low utilization, however, so the vast majority of open queue jobs can be submitted and placed in a resonable amount of time. The open queue is entirely adequate for most individual users and for many use cases.","title":"Open Queue"},{"location":"03_SubmittingJobs/#compute-allocations","text":"A paid compute allocation provides access to specific compute resources for an individual user or for a group of users. A paid compute allocation provides the following benefits: Guaranteed job start time within one hour No job preemption for non-burst jobs Burst capability up to 4x of the allocation's compute resources A compute allocation results in the creation of a compute account on RC. The mybalance command on RC lists accessible compute accounts and resource information associated with those compute accounts. Use the mybalance -h option for additional command usage information. To submit a job to a paid compute account, supply the -A or --account resource directive with the compute account name and supply the -p or --partition resource directive with sla-prio : #SBATCH -A <compute_account> #SBATCH -p sla-prio To enable bursting, if enabled for the compute account, supply the -p or --partition resource directive with the desired level of bursting for the job ( burst2x , burst3x , burst4x , and so on). To list the available compute accounts and the associated available burst partitions, use the following command: $ sacctmgr show User $(whoami) --associations format=account%30,qos%40","title":"Compute Allocations"},{"location":"03_SubmittingJobs/#compute-resources-available","text":"A paid compute allocation will typically cover a certain number of cores across a certain timeframe. The resources associated with a compute allocation are in units of core-hours . The compute allocation has an associated Limit in core-hours based on the initial compute allocation agreement. Any amount of compute resources used on the compute allocation results in an accrual of the compute allocation's Usage , again in core-hours . The compute allocation's Balance is simply the Limit minus its Usage . Balance [core-hours] = Limit [core-hours] - Usage [core-hours] At the start of the compute allocation, 60 days-worth of compute resources are added to the compute allocation's Limit . Each day thereafter, 1 day-worth of compute resources are added to the Limit . Initial Resources [core-hours] = # cores * 24 hours/day * 60 days Daily Replenishment [core-hours] = # cores * 24 hours/day The daily replenishment scheme continues on schedule for the life of the compute allocation. Near the very end of the compute allocation, the replenishment schedule may be impacted by the enforced limit on the maximum allowable Balance . The Balance for a compute allocation cannot exceed the amount of compute resources for a window of 91 days and cannot exceed the amount usable by a 4x burst for the remaining life of the compute allocation. This limit is only relevant for the replenishment schedule nearing the very end of the compute allocation life. Max Allowable Balance [core-hours] = min( WindowMaxBalance, 4xBurstMaxBalance ) where WindowMaxBalance [core-hours] = # cores * 24 hours/day * 91 days 4xBurstMaxBalance [core-hours] = # cores * 24 hours/day * # days remaining * 4 burst factor","title":"Compute Resources Available"},{"location":"03_SubmittingJobs/#roar-collab-compute-account-self-management","text":"","title":"Roar Collab Compute Account Self-Management"},{"location":"03_SubmittingJobs/#modifying-allocation-coordinators","text":"The principal contact for a compute allocation is automatically designated as a coordinator for the compute account associated with the compute allocation. A coordinator can add or remove another coordinator with the following command: $ sacctmgr add coordinator account=<compute-account> name=<userid> $ sacctmgr remove coordinator account=<compute-account> name=<userid>","title":"Modifying Allocation Coordinators"},{"location":"03_SubmittingJobs/#adding-and-removing-users-from-an-allocation","text":"A coordinator can then add and remove users from the compute account using the following: $ sacctmgr add user account=<compute-account> name=<userid> $ sacctmgr remove user account=<compute-account> name=<userid>","title":"Adding and Removing Users from an Allocation"},{"location":"03_SubmittingJobs/#using-gpus","text":"GPUs are available on RC to users that are added to paid GPU compute accounts. To use GPUs, add the --gpus resource directive: #!/bin/bash #SBATCH --job-name=apythonjob # give the job a name #SBATCH --account=<gpu_acct> # specify the account #SBATCH --partition=sla-prio # specify the partition #SBATCH --nodes=1 # request a node #SBATCH --ntasks=1 # request a task / cpu #SBATCH --mem=1G # request the memory required per node #SBATCH --gpus=1 # request a gpu #SBATCH --time=00:01:00 # set a limit on the total run time python pyscript.py Requesting GPU resources for a job is only beneficial if the software running within the job is GPU-enabled.","title":"Using GPUs"},{"location":"03_SubmittingJobs/#job-management-and-monitoring","text":"A user can find the job ID, the assigned node(s), and other useful information using the squeue command. Specifically, the following command displays all running and queued jobs for a specific user: $ squeue -u <user> A useful environment variable is the SQUEUE_FORMAT variable which enables customization of the details shown by the squeue command. This variable can be set, for example, with the following command to provide a highly descriptive squeue output: $ export SQUEUE_FORMAT=\"%.9i %9P %35j %.8u %.2t %.12M %.12L %.5C %.7m %.4D %R\" Further details on the usage of this variable are available on Slurm's squeue documentation page. Another useful job monitoring command is: $ scontrol show job <jobid> Also, a job can be cancelled with $ scancel <jobid> Valuable information can be obtained by monitoring a job on the compute node(s) as the job runs. Connect to the compute node of a running job with the ssh command. Note that a compute node can only be reached if the user has a resource reservation on that specific node. After connecting to the compute node, the top and ps commands are useful tools. $ ssh <comp-node-id> $ top -Hu <user> $ ps -aux | grep <user>","title":"Job Management and Monitoring"},{"location":"03_SubmittingJobs/#converting-from-pbs-to-slurm","text":"Slurm's commands and scheduler directives can be mapped to and from PBS/Torque commands and scheduler directives. To convert any PBS/Torque scripts and/or workflows to Slurm, the commands and scheduler directives should be swapped out and reconfigured. See the table below for the mapping of some common commands and scheduler directives: Action PBS/Torque Command Slurm Command Submit a batch job qsub sbatch Request an interactive job qsub -I salloc Cancel a job qdel scancel Check job status qstat squeue Check job status for specific user qstat -u <user> squeue -u <user> Hold a job qhold scontrol hold Release a job qrls scontrol release Resource Request PBS/Torque Directive Slurm Directive Directive designator #PBS #SBATCH Number of nodes -l nodes -N or --nodes Number of CPUs -l ppn -n or --ntasks Amount of memory -l mem --mem or --mem-per-cpu Walltime -l walltime -t or --time Compute account -A -A or --account For a more complete list of command, scheduler directive, and option comparisons, see the Slurm Rosetta Stone .","title":"Converting from PBS to Slurm"},{"location":"04_HandlingData/","text":"Handling Data Available Filesystems and Quotas Roar Collab (RC) offers several file storage options for users, each with their own quotas and data retention policies. The multiple options are available for users to optimize their workflows. Storage Information Storage Location Space Quota Files Quota Backup Policy Use Case Home /storage/home 16 GB 500,000 files Daily snapshot Configuration files Work /storage/work 128 GB 1 million files Daily snapshot Primary user-level data Scratch /scratch None 1 million files No Backup Files purged after 30 days Temporary files Group /storage/group Specific to allocation 1 million files per TB allocated Daily snapshot Primary shared data Home should primarily be used for configuration files and should not be used as a primary storage location for data. Work should be used as the primary personal data storage location. Scratch should be used for temporary files and for reading and writing large data files. To provide a user with access to a paid group storage allocation, the owner of the storage allocation should submit a request to icds@psu.edu to add the user to their <owner>_collab group. Check Usage To check storage usage against the storage quotas, run the following command on RC: $ check_storage_quotas The ouputs generated by these scripts are not generated in real-time, but the underlying quota information is updated several times per day. After removing many files, for instance, the updates to the storage usage will not be reflected in the outputs until the next update period. For a real-time look into the memory usage for a particular storage location, navigate to the storage location and run the following command: $ du -sch .[!.]* * | sort -h For a real-time look into the number of files in a storage location, navigate to the storage location and run the following command: $ find . -type f | wc -l A user can check the storage usage of an accessible group storage location by navigating to the group storage location and running the following command: $ df -ui . Managing Large Configuration Files Home is the primary location for configuration files, and many software packages will automatically place configuration files in this location. Sometimes, these configuration files can grow in size such that the Home directory approaches its storage quota limit. If this issue occurs, it is simple to move the configuration files from Home to Work and place a link in Home that points to the new location of the configuration files in Work. For instance, Anaconda stores its configuration files in ~/.conda by default, and this directory often grows to multiple GBs in size, consequently using a significant portion of the Home directory's allocated memory. The ~/.conda directory can be moved to Work and can be replaced by a link in Home that points to the new location. This can be carried out with the following commands: $ mv ~/.conda /storage/work/$(whoami) $ ln -s /storage/work/$(whoami)/.conda ~/.conda Storage Allocations A paid storage allocation provides access to a shareable group location for active file storage. Active group storage is included with a paid compute allocation, but additional storage space can be purchased separately as well. Active storage is mounted on all compute resources and enables users to read, write, and modify files stored there. For long-term storage of infrequently- used files that is separate from compute resources, archive storage is available for purchase and is accessible via the Globus interface. Typically, access to group storage locations can be managed using the <user>_collab group where the <user> is the user ID of the owner of the group space. For users to be added or removed from <user>_collab groups, the owner of that group must submit a request to icds@psu.edu . User-Managed Groups If the owner of a group space would like more control over the access groups or would like to designate a group coordinator, then it is recommended that the owner create a User Managed Group (UMG) . The UMG allows a user to manage the group access list and group roles directly through the User Managed Group functionality through Penn State Accounts Management . Select the following options and adhere to the following recommendations when creating the UMG: Group Function: Functional Campus: University Park Display Name: icds.rc.<umg_name> e.g. icds.rc.abc1234_collab Email: Not necessary for RC use Security: Sync with Enterprise Active Directory is required ICDS filters UMGs for display names that begin with icds.rc. , so any UMGs created with this prefix will automatically appear within RC. It may take up to 15 minutes for a newly-created UMG to appear on RC. To verify that a UMG is available on RC, run the following command on RC: $ getent group icds.rc.<umg_name> After a UMG is created, the owner can submit a request to icds@psu.edu to associate this UMG with the <owner>_collab group. Once this association is made, then the group owner has full dynamic control over the access and roles of the <owner>_collab group by modifying the UMG membership. After this single request to ICDS, the owner no longer has to submit requests to modify group membership and instead can manage the group directly. Note that any user added as a UMG member that does not have an active RC account will not have access to RC or any data on RC until that user has an active RC account. File Transfers Globus is a web-based file transfer tool. With Globus, users can easily, reliably, and securely transfer data to and from RC. The recommended file transfer method for RC is Globus . The Files tab on the RC Portal is also very convenient for transferring files. For small-scale file transfers to/from RC, the submit nodes (hostname submit.hpc.psu.edu ) can be used. When specifying a file location, it is best to use the full file path. Globus It is recommended to use Globus for file transfers, especially for files that are multiple GBs in size or larger. Also, if issues due to an unreliable connection arise, transferring via Globus may be a good option. Globus is a file transfer tool that automates the activity of managing file transfers, such as monitoring performance, retrying failed transfers, recovering from faults automatically whenever possible, and reporting status. Globus endpoints must be installed on both the source and destination systems. RC has Globus endpoints available. RC Endpoint: PennState_ICDS_RC RC Archive Endpoint: Archive_PennState_ICDS To transfer files with Globus, visit the Globus website and log in as a Penn State user with your Penn State access account. Select the File Manager tab on the left side of the Globus web interface and select the source and destination endpoints. The endpoints may require an additional login. The files and locations for the transfer can be selected graphically, and the transfer can be initiated by selecting Start above the source endpoint file preview window. The transfer will be handled by Globus, and typically, successful completion of the transfer will generate an email to your Penn State email. Users can also download files from RC to their local device or upload files directly from their local device to RC using simple web interface operations. To download a file, right-click the file and select Download. To upload a file, select Upload from the Pane 1 Menu on the right. Globus provides detailed instructions on the following topics: - Log in and transfer files - Install and configure Globus Connect for Linux - Install and configure Globus Connect for MacOSX - Install and configure Globus Connect for Windows Files Tab on Roar Collab Portal The Files tab on the RC Portal offers a very intuitive interface for file management. Files can be moved, edited, uploaded, and downloaded with relative ease using this utility. Users should limit the use of the RC Portal file manager utility to dealing with small files only. scp Users may use the scp command to transfer files to and from RC. It is typically practical to zip files together when transferring many files at once. In a terminal session, the scp command can be used to transfer files in the following way: $ scp [options] <source-user-id>@<source-host>[:<file-location>] <destination-user-id>@<destination-host>[:<file-location>] Some examples will make the usage more clear. The two locations in this example are the directory /home/abc on a local laptop device and the user scratch space on RC. If a file named local.file in /home/abc is to be transferred to a user's scratch space, the user should run the following command from a terminal session on the local laptop: # Transfer to RC scratch space $ scp /home/abc/local.file <userid>@submit.hpc.psu.edu:/scratch/<userid> Alternatively, if the user navigates to the /home/abc location on their local laptop, the command can be slightly simplified to # Transfer to RC scratch space $ scp local.file <userid>@submit.hpc.psu.edu:/scratch/<userid> If a directory named datadir located on the user's RC scratch space is to be transferred to the local laptop's /home/abc directory, the user can run the following from a terminal session on the local laptop: # Transfer from RC scratch space $ scp -r <userid>@submit.hpc.psu.edu:/scratch/<userid> /home/abc/ Note that since a directory is being transferred, the -r option must be used for the scp command so both the directory and its contents are transferred. If the user navigates the terminal to the /home/abc directory to conduct the transfer, then the /home/abc/ in the above commands can be replaced with a single period . to denote the current working directory . # Transfer from RC scratch space $ scp -r <userid>@submit.hpc.psu.edu:/scratch/<userid> . sftp The sftp command can also be used to transfer files and is more useful when transferring multiple smaller files in a piecemeal fashion. This method allows for interactive navigation on the remote connection. From a local device, an sftp connection can be made with $ sftp <userid>@<remote-host>[:<location>] To spawn an sftp connection on RC, use $ sftp <userid>@submit.hpc.psu.edu After the sftp connection is made, navigational commands (i.e. ls , cd , etc) are performed on the remote connection normally, while navigational commands are performed on the local connection by appending the letter l (lowercase L) to the commands (i.e. lls , lcd , etc). Files are transferred from the local device to the remote device using the put <filename> command, and files are transferred from the remote device to the local device using the get <filename> command. The connection is terminated with the exit command. rsync Yet another file transfer option is rsync . The rsync tool is widely used for backups and mirroring and as an improved copy command for everyday use. The rsync command takes the form $ rsync [options] <source-user-id>@<source-host>[:<file-location>] <destination-user-id>@<destination-host>[:<file-location>] The rsync tool should only be used within an interactive compute session due to its underlying resource requirements.","title":"Handling Data"},{"location":"04_HandlingData/#handling-data","text":"","title":"Handling Data"},{"location":"04_HandlingData/#available-filesystems-and-quotas","text":"Roar Collab (RC) offers several file storage options for users, each with their own quotas and data retention policies. The multiple options are available for users to optimize their workflows.","title":"Available Filesystems and Quotas"},{"location":"04_HandlingData/#storage-information","text":"Storage Location Space Quota Files Quota Backup Policy Use Case Home /storage/home 16 GB 500,000 files Daily snapshot Configuration files Work /storage/work 128 GB 1 million files Daily snapshot Primary user-level data Scratch /scratch None 1 million files No Backup Files purged after 30 days Temporary files Group /storage/group Specific to allocation 1 million files per TB allocated Daily snapshot Primary shared data Home should primarily be used for configuration files and should not be used as a primary storage location for data. Work should be used as the primary personal data storage location. Scratch should be used for temporary files and for reading and writing large data files. To provide a user with access to a paid group storage allocation, the owner of the storage allocation should submit a request to icds@psu.edu to add the user to their <owner>_collab group.","title":"Storage Information"},{"location":"04_HandlingData/#check-usage","text":"To check storage usage against the storage quotas, run the following command on RC: $ check_storage_quotas The ouputs generated by these scripts are not generated in real-time, but the underlying quota information is updated several times per day. After removing many files, for instance, the updates to the storage usage will not be reflected in the outputs until the next update period. For a real-time look into the memory usage for a particular storage location, navigate to the storage location and run the following command: $ du -sch .[!.]* * | sort -h For a real-time look into the number of files in a storage location, navigate to the storage location and run the following command: $ find . -type f | wc -l A user can check the storage usage of an accessible group storage location by navigating to the group storage location and running the following command: $ df -ui .","title":"Check Usage"},{"location":"04_HandlingData/#managing-large-configuration-files","text":"Home is the primary location for configuration files, and many software packages will automatically place configuration files in this location. Sometimes, these configuration files can grow in size such that the Home directory approaches its storage quota limit. If this issue occurs, it is simple to move the configuration files from Home to Work and place a link in Home that points to the new location of the configuration files in Work. For instance, Anaconda stores its configuration files in ~/.conda by default, and this directory often grows to multiple GBs in size, consequently using a significant portion of the Home directory's allocated memory. The ~/.conda directory can be moved to Work and can be replaced by a link in Home that points to the new location. This can be carried out with the following commands: $ mv ~/.conda /storage/work/$(whoami) $ ln -s /storage/work/$(whoami)/.conda ~/.conda","title":"Managing Large Configuration Files"},{"location":"04_HandlingData/#storage-allocations","text":"A paid storage allocation provides access to a shareable group location for active file storage. Active group storage is included with a paid compute allocation, but additional storage space can be purchased separately as well. Active storage is mounted on all compute resources and enables users to read, write, and modify files stored there. For long-term storage of infrequently- used files that is separate from compute resources, archive storage is available for purchase and is accessible via the Globus interface. Typically, access to group storage locations can be managed using the <user>_collab group where the <user> is the user ID of the owner of the group space. For users to be added or removed from <user>_collab groups, the owner of that group must submit a request to icds@psu.edu .","title":"Storage Allocations"},{"location":"04_HandlingData/#user-managed-groups","text":"If the owner of a group space would like more control over the access groups or would like to designate a group coordinator, then it is recommended that the owner create a User Managed Group (UMG) . The UMG allows a user to manage the group access list and group roles directly through the User Managed Group functionality through Penn State Accounts Management . Select the following options and adhere to the following recommendations when creating the UMG: Group Function: Functional Campus: University Park Display Name: icds.rc.<umg_name> e.g. icds.rc.abc1234_collab Email: Not necessary for RC use Security: Sync with Enterprise Active Directory is required ICDS filters UMGs for display names that begin with icds.rc. , so any UMGs created with this prefix will automatically appear within RC. It may take up to 15 minutes for a newly-created UMG to appear on RC. To verify that a UMG is available on RC, run the following command on RC: $ getent group icds.rc.<umg_name> After a UMG is created, the owner can submit a request to icds@psu.edu to associate this UMG with the <owner>_collab group. Once this association is made, then the group owner has full dynamic control over the access and roles of the <owner>_collab group by modifying the UMG membership. After this single request to ICDS, the owner no longer has to submit requests to modify group membership and instead can manage the group directly. Note that any user added as a UMG member that does not have an active RC account will not have access to RC or any data on RC until that user has an active RC account.","title":"User-Managed Groups"},{"location":"04_HandlingData/#file-transfers","text":"Globus is a web-based file transfer tool. With Globus, users can easily, reliably, and securely transfer data to and from RC. The recommended file transfer method for RC is Globus . The Files tab on the RC Portal is also very convenient for transferring files. For small-scale file transfers to/from RC, the submit nodes (hostname submit.hpc.psu.edu ) can be used. When specifying a file location, it is best to use the full file path.","title":"File Transfers"},{"location":"04_HandlingData/#globus","text":"It is recommended to use Globus for file transfers, especially for files that are multiple GBs in size or larger. Also, if issues due to an unreliable connection arise, transferring via Globus may be a good option. Globus is a file transfer tool that automates the activity of managing file transfers, such as monitoring performance, retrying failed transfers, recovering from faults automatically whenever possible, and reporting status. Globus endpoints must be installed on both the source and destination systems. RC has Globus endpoints available. RC Endpoint: PennState_ICDS_RC RC Archive Endpoint: Archive_PennState_ICDS To transfer files with Globus, visit the Globus website and log in as a Penn State user with your Penn State access account. Select the File Manager tab on the left side of the Globus web interface and select the source and destination endpoints. The endpoints may require an additional login. The files and locations for the transfer can be selected graphically, and the transfer can be initiated by selecting Start above the source endpoint file preview window. The transfer will be handled by Globus, and typically, successful completion of the transfer will generate an email to your Penn State email. Users can also download files from RC to their local device or upload files directly from their local device to RC using simple web interface operations. To download a file, right-click the file and select Download. To upload a file, select Upload from the Pane 1 Menu on the right. Globus provides detailed instructions on the following topics: - Log in and transfer files - Install and configure Globus Connect for Linux - Install and configure Globus Connect for MacOSX - Install and configure Globus Connect for Windows","title":"Globus"},{"location":"04_HandlingData/#files-tab-on-roar-collab-portal","text":"The Files tab on the RC Portal offers a very intuitive interface for file management. Files can be moved, edited, uploaded, and downloaded with relative ease using this utility. Users should limit the use of the RC Portal file manager utility to dealing with small files only.","title":"Files Tab on Roar Collab Portal"},{"location":"04_HandlingData/#scp","text":"Users may use the scp command to transfer files to and from RC. It is typically practical to zip files together when transferring many files at once. In a terminal session, the scp command can be used to transfer files in the following way: $ scp [options] <source-user-id>@<source-host>[:<file-location>] <destination-user-id>@<destination-host>[:<file-location>] Some examples will make the usage more clear. The two locations in this example are the directory /home/abc on a local laptop device and the user scratch space on RC. If a file named local.file in /home/abc is to be transferred to a user's scratch space, the user should run the following command from a terminal session on the local laptop: # Transfer to RC scratch space $ scp /home/abc/local.file <userid>@submit.hpc.psu.edu:/scratch/<userid> Alternatively, if the user navigates to the /home/abc location on their local laptop, the command can be slightly simplified to # Transfer to RC scratch space $ scp local.file <userid>@submit.hpc.psu.edu:/scratch/<userid> If a directory named datadir located on the user's RC scratch space is to be transferred to the local laptop's /home/abc directory, the user can run the following from a terminal session on the local laptop: # Transfer from RC scratch space $ scp -r <userid>@submit.hpc.psu.edu:/scratch/<userid> /home/abc/ Note that since a directory is being transferred, the -r option must be used for the scp command so both the directory and its contents are transferred. If the user navigates the terminal to the /home/abc directory to conduct the transfer, then the /home/abc/ in the above commands can be replaced with a single period . to denote the current working directory . # Transfer from RC scratch space $ scp -r <userid>@submit.hpc.psu.edu:/scratch/<userid> .","title":"scp"},{"location":"04_HandlingData/#sftp","text":"The sftp command can also be used to transfer files and is more useful when transferring multiple smaller files in a piecemeal fashion. This method allows for interactive navigation on the remote connection. From a local device, an sftp connection can be made with $ sftp <userid>@<remote-host>[:<location>] To spawn an sftp connection on RC, use $ sftp <userid>@submit.hpc.psu.edu After the sftp connection is made, navigational commands (i.e. ls , cd , etc) are performed on the remote connection normally, while navigational commands are performed on the local connection by appending the letter l (lowercase L) to the commands (i.e. lls , lcd , etc). Files are transferred from the local device to the remote device using the put <filename> command, and files are transferred from the remote device to the local device using the get <filename> command. The connection is terminated with the exit command.","title":"sftp"},{"location":"04_HandlingData/#rsync","text":"Yet another file transfer option is rsync . The rsync tool is widely used for backups and mirroring and as an improved copy command for everyday use. The rsync command takes the form $ rsync [options] <source-user-id>@<source-host>[:<file-location>] <destination-user-id>@<destination-host>[:<file-location>] The rsync tool should only be used within an interactive compute session due to its underlying resource requirements.","title":"rsync"},{"location":"05_UsingSoftware/","text":"Using Software The software stack on Roar Collab (RC) provides a wide variety of software to the entire user community. There are two software stacks available on RC. System software stack : contains software that is available to all users by default upon logging into the system without a need to load anything. Central software stack : contains software that is available to all users by default, but the software modules must be loaded in order to access them. Modules The central software stack uses Lmod to package the available software. Lmod is a useful tool for managing user software environments using environment modules that can be dynamically added or removed using module files. Lmod is hierarchical, so sub-modules can be nested under a module that is dependent upon. Lmod alters environment variables, most notably the $PATH variable, in order to make certain software packages reachable by the user environment. Useful Lmod Commands Command Description module avail List all modules that are available to be loaded module show <module_name> Show the contents of a module module spider <module_name> Search the module space for a match module load <module_name> Load module(s) module load <module>/<version> Load a module of a specific version module unload <module_name> Unload module(s) module list List all currently loaded modules module purge Unload all currently loaded modules module use <path> Add a path to $MODULEPATH to expand module scope module unuse <path> Remove a path from $MODULEPATH The central software stack is available to the user environment by default since the $MODULEPATH environment variable is set and contains the central software stack location. Modules can be directly loaded with $ module load <package> To see the available software modules, use $ module avail Custom Software Even though a large variety of software packages are available on RC via the system and central software stacks, users may need access to additional software. Users may also wish to have greater control over the software packages that are required for their research workflow. Custom Modules Users can install custom software packages and build custom software modules in order to build a custom user- or group-specific software stack. For users and groups with well-defined research workflows, it is recommended to create a custom software stack to keep close control of the software installation versions and configuration. A location should be specified that contains the custom software installations and the module files for the custom software installations should be stored together in a common location. This module location can be added to the $MODULEPATH environment variable so users can access the software modules just as they would for the central software stack. The LMod documentation contains more detailed information on creating custom software modules. Anaconda Anaconda is a very useful package manager that is available on RC. Package managers simplify software package installation and manage dependency relationships while increasing both the repeatability and the portability of software. The user environment is modified by the package manager so the shell can access different software packages. Anaconda was originally created for Python, but it can package and distribute software for any language. It is usually very simple to create and manage new environments, install new packages, and import/export environments. Many packages are available for installation through Anaconda, and it enables retaining the environments in a silo to reduce cross-dependencies between different packages that may perturb environments. Anaconda can be loaded from the software stack on RC with the following command: $ module load anaconda Usage of Anaconda may cause storage quota issues since environments and packages are stored within ~/.conda by default. This issue can be easily resolved by moving the ~/.conda directory to the work directory and creating a link in its place pointing to the new location in the work directory. This is described further in the Handling Data section. Installation Example After loading the anaconda module, environments can be created and packages can be installed within those environments. When using the anaconda module for the first time on RC, the conda init bash command may be required to initialize anaconda, then a new session must be started for the change to take effect. In the new session, the command prompt will be prepended with (base) which denotes that the session is in the base anaconda environment. To create an environment that contains both numpy and scipy, for example, run the following commands: (base) $ conda create -n py_env (base) $ conda activate py_env (py_env) $ conda install numpy (py_env) $ conda install scipy Note that after the environment is entered, the leading item in the prompt changes to reflect the current environment. Alternatively, the creation of an environment and package installation can be completed with a single line. (base) $ conda create -n py_env numpy scipy For more detailed information on usage, check out the Anaconda documentation . Useful Anaconda Commands Command Description conda create \u2013n <env_name> Creates a conda environment by name conda create \u2013p <env_path> Creates a conda environment by location conda env list Lists all conda environments conda env remove \u2013n <env_name> Removes a conda environment by name conda activate <env_name> Activates a conda environment by name conda list Lists all packages within an active environment conda deactivate Deactivates the active conda environment conda install <package> Installs a package within an active environment conda search <package> Searches for a package conda env export > env_name.yml Exports active environment to a file conda env create \u2013f env_name.yml Loads environment from a file Submission Script Usage Slurm does not automatically source the ~/.bashrc file in your batch job, so Anaconda fails to be properly initialized within Slurm job submission scripts. Fortunately, the anaconda module intitializes the software so that the conda command is automatically available within the Slurm job submission script. If using a different anaconda installation, this issue can be resolved by directly sourcing the ~/.bashrc file in your job script before running any conda commands: source ~/.bashrc Alternatively, the environment can be activated using source instead of conda . source activate <environment> Another way to resolve this is to add the following shebang to the top of a slurm job script: #!/usr/bin/env bash -l Yet another option would be to put the following commands before activating the conda environment: module load <custom anaconda module> CONDAPATH=`which conda` eval \"$(${CONDAPATH} shell.bash hook)\" To reiterate, the anaconda module available on RC is configured such that the conda command is automatically available within a Slurm job submission sript. The above options are only necessary for other anaconda installations. Using Conda Environments in Interactive Apps Environments built with Anaconda can be used in Interactive Apps on the RC Portal as well. Typically the environment should be created and configured in an interactive compute session on RC, and then some additional steps are needed to make the environment available from within an Interactive App. Jupyter Server To access a conda environment within a Jupyter Server session, the ipykernel package must be installed within the environment. To do so, enter the environment and run the following commands: (base) $ conda activate <environment> (<environment>) $ conda install -y ipykernel (<environment>) $ ipython kernel install --user --name=<environment> After the ipykernel package is successfully installed within this environment, a Jupyter Server session can be launched via the RC Portal. When submitting the form to launch the session, under the Conda environment type field, select the Use custom text field option from the dropdown menu. Then enter the following into the Environment Setup text field: module load anaconda After launching and entering the session, the environment is displayed in the kernel list. RStudio To launch an RStudio Interactive App session, RStudio must have access to an installation of R. R can either be installed within the conda environment itself, or it can be loaded from the software stack. Typically, R will be installed by default when installing R packages within a conda environment; therefore, it is recommended when using conda environments within RStudio to simply utilize the environment's own R installation. To create an environment containing an R installation, run the following command: (base) $ conda create -y -n <environment> r Alternatively, R can simply be added to an existing environment by entering that environment and installing using the following command: (<environment>) $ conda install r <plus any additional R packages> R packages can installed directly via Anaconda within the environment as well. R packages available in Anaconda are usually named r-<package name> such as r-plot3d , r-spatial , or r-ggplot . After R and any necessary R packages are installed within the environment, an RStudio session can be launched via the RC Portal. When submitting the form to launch the session, under the Environment type field, select the Use custom text field option from the dropdown menu. Then enter the following into the Environment Setup text field: module load anaconda conda activate <environment> export CONDAENVLIB=~/.conda/envs/<environment>/lib export LD_LIBRARY_PATH=$CONDAENVLIB:$LD_LIBRARY_PATH Please note that the default location of conda environments is in ~/.conda/envs , which is why the CONDAENVLIB variable is being set to ~/.conda/envs/<environment>/lib . If the environment is instead installed a non-default location, then the CONDAENVLIB variable should be set accordingly. The two export commands in the block above are required because RStudio often has an issue loading some libraries while accessing the conda envrionment's R installation. Explicitly adding the conda environment's lib directory to the LD_LIBRARY_PATH variable seems to clear up this issue. Compiling From Source Compiling software from source is the most involved option for using software on RC, but it gives the user the highest level of control. Research computing software is often developed by academic researchers that do not place a large effort on packaging their software so that it can be easily deployed on other systems. If the developer does not package the software using a package manager, then the only option is to build the software from source. It is best to follow the installation instructions from the developer to successfully install the software from source. It is recommended to build software on a node with the same processor type that will be used for running the software. On a compute node, running the following command displays the processor type: $ cat /sys/devices/cpu/caps/pmu_name Software builds are not typically back-compatible and will not run successfully on processors older than the processor used to build. It is recommended to build on haswell (the oldest processor architecture on RC) if you wish to have full compatibility across all RC compute nodes. To optimize for performance, however, build on the same processor on which the software runs. | Release Date | Processor | | :----: | :----: | | 2013 | haswell | | 2014 | broadwell | | 2015 | skylake | | 2019 | cascadelake | | 2019 | icelake | | 2023 | sapphirerapids | Containers A container is a standard unit of software with two modes: Idle: When idle, a container is a file that stores everything an application (or collection of applications) requires to run (code, runtime, system tools, system libraries and settings). Running: When running, a container is a Linux process running on top of the host machine kernel with a user environment defined by the contents of the container file, not by the host OS. A container is an abstraction at the application layer. Multiple containers can run on the same machine and share the host kernel with other containers, each running as isolated processes. Apptainer is a secure container platform designed for HPC use cases and is available on RC. Containers (or images) can either be pulled directly from a container repository or can be built from a definition file. A definition file or recipe file contains everything required to build a container. Building containers requires root privileges, so containers are built on your personal device and can be deployed on RC. Software is continuously growing in complexity which can make managing the required user environment and wrangling dependent software an intractable problem. Containers address this issue by storing the software and all of its dependencies (including a minimal operating system) in a single image file, eliminating the need to install additional packages or alter the runtime environment. This makes the software both shareable and portable while the output becomes reproducible. In a Slurm submission script, a container can be called serially using the following run line: $ apptainer run <container> <args> To use a container in parallel with MPI, the MPI library within the container must be compatible with the MPI implementation on the system, meaning that the MPI version on the system must generally be newer than the MPI version within the container. More details on using MPI with containers can be found on Apptainer's Apptainer and MPI Applications page. In a Slurm submission script, a container with MPI can be called using $ srun apptainer exec <container> <command> <args> Containers change the user space into a swappable component, and provide the following benefits: Flexibility: Bring your own environment (BYOE) and bring your own software (BYOS) Reproducibility: Complete control over software versions Portability: Run a container on your laptop or on HPC systems Performance: Similar performance characteristics as native applications Compatibility: Open standard that is supported on all major Linux distributions Container Registries Container images can be made publicly available, and containers for many use cases can be found at the following container registries: Docker Hub Singularity Hub Singularity Cloud Library NVIDIA GPU Cloud Quay.io BioContainers Useful Apptainer Commands Command Description apptainer build <container> <definition> Builds a container from a definition file apptainer shell <container> Runs a shell within a container apptainer exec <container> <command> Runs a command within a container apptainer run <container> Runs a container where a runscript is defined apptainer pull <resource>://<container> Pulls a container from a container registry apptainer build --sandbox <sbox> <container> Builds a sandbox from a container apptainer build <container> <sbox> Builds a container from a sandbox Building Container Images Containers can be made from scratch using a definition file , or recipe file, which is a text file that specifies the base image, the software to be installed, and other information. The documentation for the apptainer build command shows the full usage for the build command. Container images can also be bootstrapped from other images, found on Docker Hub for instance. The recommended workflow for building containers is shown below: Software-Specific Guides Python Python is a high-level, general-purpose programming language. Python versions on RC Python is available by default to all users on the system software stack, and it is also available on the central software stack. Additionally, users can install their own instances of Python in a variety of ways in either their userspace or in group spaces. Install Python Packages with pip Python packages can be installed easily using pip . By default, pip will attempt to install packages to a system location, but RC is a shared system. Users do not have write access to system locations on shared systems. The packages can instead be installed in ~/.local , which is a user location, using the following: $ pip install --user <package> Also, packages can be installed to a custom specified location using the --target option: $ pip install --target=<install_dir> <package> Note that if pip is not available, simply try pip3 (for python3) or pip2 (for python2) instead. R R is a free software environment for statistical computing and graphics. R Versions on RC R users should make sure that the version of R remains consistent. RC has several R versions available, and when a package is installed in one version, it is not always accessible when operating in another version. Always check the R version and remain consistent! R modules can be loaded from the central software stack, and R can also be installed by users in a variety of ways within their userspace or group spaces. Install R Packages R manages some dependencies and versions through the CRAN-like repos. R packages can be installed from within the R console with the following command: > install.packages( <package> ) Upon running the install command, a warning usually appears stating that the default system install location is not writable, so it asks to install in a personal library instead. After entering \"yes\" as a response, it may then ask to create a personal library location. Responding \"yes\" again will proceed with the installation, probably by asking to select a CRAN repository. The default personal directory described above will install the package in the ~/R/ directory. An install location can instead be supplied to the install command using the lib argument: > install.packages( \"<package>\", lib=\"<install_location>\" ) After installation, packages can then be loaded using the following command in the R console: > library( <package> ) If the package was installed in a non-standard location, then the package can be loaded from that custom install location using the lic.loc argument of the library() command: > library( <package>, lib.loc=\"<install_location>\" ) It is recommended to review dependencies of any packages to be installed because additional software may have to be loaded in the environment before launching the R console. For example, some R packages utilize CMake to perform the installation. In that case, the cmake module should be loaded before launching the R console session. R Package Installation Example To install the ggplot2 R package, first search ggplot2 online to see if there are installation instructions. A quick search shows that ggplot2 is included in the tidyverse package and that the recommended installation instructions are the following: # The easiest ways to get ggplot2 is to install the whole tidyverse package: > install.packages(\"tidyverse\") # Alternatively, install just ggplot2: > install.packages(\"ggplot2\") Searching for install instructions usually provides all the necessary information! Some R packages may require changes to the user environment before the package can be installed successfully within the R console. Typically, the user environment change is as simple as accessing a newer compiler version by loading a software module like intel with $ module load intel Sometimes, installing R packages may be a little more involved. To install the units R package, for example, an additional library must be downloaded and installed locally in order for the package to be installed properly. To install the units R package on RC, perform the following commands in an interactive session on a compute node: $ cd ~/scratch $ wget https://downloads.unidata.ucar.edu/udunits/2.2.28/udunits-2.2.28.tar.gz $ tar -xvf udunits-2.2.28.tar.gz $ cd udunits-2.2.28 $ ./configure prefix=$HOME/.local $ make $ make install $ export UDUNITS2_INCLUDE=$HOME/.local/include $ export UDUNITS2_LIBS=$HOME/.local/lib $ export LD_LIBRARY_PATH=$HOME/.local/lib:$LD_LIBRARY_PATH $ module load r/4.2.1 $ R > install.packages(\"units\") > library(units) R Packages with Anaconda The R installation itself and its R packages can be easily installed and managed within an conda environment. Creating a conda environment containing its own R installation and some R packages can be accomplished with the following: (base) $ conda create -n r_env (base) $ conda activate r_env (r_env) $ conda install r-base (r_env) $ conda install r-tidyverse Note that after r-base is the base R installation, and R packages (or in the case of tidyverse , a bundle of packages) usually are named r-* in conda repos. Alternatively, the creation of this environment can be completed with a single line. (base) $ conda create -n r_env r-base r-tidyverse","title":"Using Software"},{"location":"05_UsingSoftware/#using-software","text":"The software stack on Roar Collab (RC) provides a wide variety of software to the entire user community. There are two software stacks available on RC. System software stack : contains software that is available to all users by default upon logging into the system without a need to load anything. Central software stack : contains software that is available to all users by default, but the software modules must be loaded in order to access them.","title":"Using Software"},{"location":"05_UsingSoftware/#modules","text":"The central software stack uses Lmod to package the available software. Lmod is a useful tool for managing user software environments using environment modules that can be dynamically added or removed using module files. Lmod is hierarchical, so sub-modules can be nested under a module that is dependent upon. Lmod alters environment variables, most notably the $PATH variable, in order to make certain software packages reachable by the user environment.","title":"Modules"},{"location":"05_UsingSoftware/#useful-lmod-commands","text":"Command Description module avail List all modules that are available to be loaded module show <module_name> Show the contents of a module module spider <module_name> Search the module space for a match module load <module_name> Load module(s) module load <module>/<version> Load a module of a specific version module unload <module_name> Unload module(s) module list List all currently loaded modules module purge Unload all currently loaded modules module use <path> Add a path to $MODULEPATH to expand module scope module unuse <path> Remove a path from $MODULEPATH The central software stack is available to the user environment by default since the $MODULEPATH environment variable is set and contains the central software stack location. Modules can be directly loaded with $ module load <package> To see the available software modules, use $ module avail","title":"Useful Lmod Commands"},{"location":"05_UsingSoftware/#custom-software","text":"Even though a large variety of software packages are available on RC via the system and central software stacks, users may need access to additional software. Users may also wish to have greater control over the software packages that are required for their research workflow.","title":"Custom Software"},{"location":"05_UsingSoftware/#custom-modules","text":"Users can install custom software packages and build custom software modules in order to build a custom user- or group-specific software stack. For users and groups with well-defined research workflows, it is recommended to create a custom software stack to keep close control of the software installation versions and configuration. A location should be specified that contains the custom software installations and the module files for the custom software installations should be stored together in a common location. This module location can be added to the $MODULEPATH environment variable so users can access the software modules just as they would for the central software stack. The LMod documentation contains more detailed information on creating custom software modules.","title":"Custom Modules"},{"location":"05_UsingSoftware/#anaconda","text":"Anaconda is a very useful package manager that is available on RC. Package managers simplify software package installation and manage dependency relationships while increasing both the repeatability and the portability of software. The user environment is modified by the package manager so the shell can access different software packages. Anaconda was originally created for Python, but it can package and distribute software for any language. It is usually very simple to create and manage new environments, install new packages, and import/export environments. Many packages are available for installation through Anaconda, and it enables retaining the environments in a silo to reduce cross-dependencies between different packages that may perturb environments. Anaconda can be loaded from the software stack on RC with the following command: $ module load anaconda Usage of Anaconda may cause storage quota issues since environments and packages are stored within ~/.conda by default. This issue can be easily resolved by moving the ~/.conda directory to the work directory and creating a link in its place pointing to the new location in the work directory. This is described further in the Handling Data section.","title":"Anaconda"},{"location":"05_UsingSoftware/#installation-example","text":"After loading the anaconda module, environments can be created and packages can be installed within those environments. When using the anaconda module for the first time on RC, the conda init bash command may be required to initialize anaconda, then a new session must be started for the change to take effect. In the new session, the command prompt will be prepended with (base) which denotes that the session is in the base anaconda environment. To create an environment that contains both numpy and scipy, for example, run the following commands: (base) $ conda create -n py_env (base) $ conda activate py_env (py_env) $ conda install numpy (py_env) $ conda install scipy Note that after the environment is entered, the leading item in the prompt changes to reflect the current environment. Alternatively, the creation of an environment and package installation can be completed with a single line. (base) $ conda create -n py_env numpy scipy For more detailed information on usage, check out the Anaconda documentation .","title":"Installation Example"},{"location":"05_UsingSoftware/#useful-anaconda-commands","text":"Command Description conda create \u2013n <env_name> Creates a conda environment by name conda create \u2013p <env_path> Creates a conda environment by location conda env list Lists all conda environments conda env remove \u2013n <env_name> Removes a conda environment by name conda activate <env_name> Activates a conda environment by name conda list Lists all packages within an active environment conda deactivate Deactivates the active conda environment conda install <package> Installs a package within an active environment conda search <package> Searches for a package conda env export > env_name.yml Exports active environment to a file conda env create \u2013f env_name.yml Loads environment from a file","title":"Useful Anaconda Commands"},{"location":"05_UsingSoftware/#submission-script-usage","text":"Slurm does not automatically source the ~/.bashrc file in your batch job, so Anaconda fails to be properly initialized within Slurm job submission scripts. Fortunately, the anaconda module intitializes the software so that the conda command is automatically available within the Slurm job submission script. If using a different anaconda installation, this issue can be resolved by directly sourcing the ~/.bashrc file in your job script before running any conda commands: source ~/.bashrc Alternatively, the environment can be activated using source instead of conda . source activate <environment> Another way to resolve this is to add the following shebang to the top of a slurm job script: #!/usr/bin/env bash -l Yet another option would be to put the following commands before activating the conda environment: module load <custom anaconda module> CONDAPATH=`which conda` eval \"$(${CONDAPATH} shell.bash hook)\" To reiterate, the anaconda module available on RC is configured such that the conda command is automatically available within a Slurm job submission sript. The above options are only necessary for other anaconda installations.","title":"Submission Script Usage"},{"location":"05_UsingSoftware/#using-conda-environments-in-interactive-apps","text":"Environments built with Anaconda can be used in Interactive Apps on the RC Portal as well. Typically the environment should be created and configured in an interactive compute session on RC, and then some additional steps are needed to make the environment available from within an Interactive App.","title":"Using Conda Environments in Interactive Apps"},{"location":"05_UsingSoftware/#jupyter-server","text":"To access a conda environment within a Jupyter Server session, the ipykernel package must be installed within the environment. To do so, enter the environment and run the following commands: (base) $ conda activate <environment> (<environment>) $ conda install -y ipykernel (<environment>) $ ipython kernel install --user --name=<environment> After the ipykernel package is successfully installed within this environment, a Jupyter Server session can be launched via the RC Portal. When submitting the form to launch the session, under the Conda environment type field, select the Use custom text field option from the dropdown menu. Then enter the following into the Environment Setup text field: module load anaconda After launching and entering the session, the environment is displayed in the kernel list.","title":"Jupyter Server"},{"location":"05_UsingSoftware/#rstudio","text":"To launch an RStudio Interactive App session, RStudio must have access to an installation of R. R can either be installed within the conda environment itself, or it can be loaded from the software stack. Typically, R will be installed by default when installing R packages within a conda environment; therefore, it is recommended when using conda environments within RStudio to simply utilize the environment's own R installation. To create an environment containing an R installation, run the following command: (base) $ conda create -y -n <environment> r Alternatively, R can simply be added to an existing environment by entering that environment and installing using the following command: (<environment>) $ conda install r <plus any additional R packages> R packages can installed directly via Anaconda within the environment as well. R packages available in Anaconda are usually named r-<package name> such as r-plot3d , r-spatial , or r-ggplot . After R and any necessary R packages are installed within the environment, an RStudio session can be launched via the RC Portal. When submitting the form to launch the session, under the Environment type field, select the Use custom text field option from the dropdown menu. Then enter the following into the Environment Setup text field: module load anaconda conda activate <environment> export CONDAENVLIB=~/.conda/envs/<environment>/lib export LD_LIBRARY_PATH=$CONDAENVLIB:$LD_LIBRARY_PATH Please note that the default location of conda environments is in ~/.conda/envs , which is why the CONDAENVLIB variable is being set to ~/.conda/envs/<environment>/lib . If the environment is instead installed a non-default location, then the CONDAENVLIB variable should be set accordingly. The two export commands in the block above are required because RStudio often has an issue loading some libraries while accessing the conda envrionment's R installation. Explicitly adding the conda environment's lib directory to the LD_LIBRARY_PATH variable seems to clear up this issue.","title":"RStudio"},{"location":"05_UsingSoftware/#compiling-from-source","text":"Compiling software from source is the most involved option for using software on RC, but it gives the user the highest level of control. Research computing software is often developed by academic researchers that do not place a large effort on packaging their software so that it can be easily deployed on other systems. If the developer does not package the software using a package manager, then the only option is to build the software from source. It is best to follow the installation instructions from the developer to successfully install the software from source. It is recommended to build software on a node with the same processor type that will be used for running the software. On a compute node, running the following command displays the processor type: $ cat /sys/devices/cpu/caps/pmu_name Software builds are not typically back-compatible and will not run successfully on processors older than the processor used to build. It is recommended to build on haswell (the oldest processor architecture on RC) if you wish to have full compatibility across all RC compute nodes. To optimize for performance, however, build on the same processor on which the software runs. | Release Date | Processor | | :----: | :----: | | 2013 | haswell | | 2014 | broadwell | | 2015 | skylake | | 2019 | cascadelake | | 2019 | icelake | | 2023 | sapphirerapids |","title":"Compiling From Source"},{"location":"05_UsingSoftware/#containers","text":"A container is a standard unit of software with two modes: Idle: When idle, a container is a file that stores everything an application (or collection of applications) requires to run (code, runtime, system tools, system libraries and settings). Running: When running, a container is a Linux process running on top of the host machine kernel with a user environment defined by the contents of the container file, not by the host OS. A container is an abstraction at the application layer. Multiple containers can run on the same machine and share the host kernel with other containers, each running as isolated processes. Apptainer is a secure container platform designed for HPC use cases and is available on RC. Containers (or images) can either be pulled directly from a container repository or can be built from a definition file. A definition file or recipe file contains everything required to build a container. Building containers requires root privileges, so containers are built on your personal device and can be deployed on RC. Software is continuously growing in complexity which can make managing the required user environment and wrangling dependent software an intractable problem. Containers address this issue by storing the software and all of its dependencies (including a minimal operating system) in a single image file, eliminating the need to install additional packages or alter the runtime environment. This makes the software both shareable and portable while the output becomes reproducible. In a Slurm submission script, a container can be called serially using the following run line: $ apptainer run <container> <args> To use a container in parallel with MPI, the MPI library within the container must be compatible with the MPI implementation on the system, meaning that the MPI version on the system must generally be newer than the MPI version within the container. More details on using MPI with containers can be found on Apptainer's Apptainer and MPI Applications page. In a Slurm submission script, a container with MPI can be called using $ srun apptainer exec <container> <command> <args> Containers change the user space into a swappable component, and provide the following benefits: Flexibility: Bring your own environment (BYOE) and bring your own software (BYOS) Reproducibility: Complete control over software versions Portability: Run a container on your laptop or on HPC systems Performance: Similar performance characteristics as native applications Compatibility: Open standard that is supported on all major Linux distributions","title":"Containers"},{"location":"05_UsingSoftware/#container-registries","text":"Container images can be made publicly available, and containers for many use cases can be found at the following container registries: Docker Hub Singularity Hub Singularity Cloud Library NVIDIA GPU Cloud Quay.io BioContainers","title":"Container Registries"},{"location":"05_UsingSoftware/#useful-apptainer-commands","text":"Command Description apptainer build <container> <definition> Builds a container from a definition file apptainer shell <container> Runs a shell within a container apptainer exec <container> <command> Runs a command within a container apptainer run <container> Runs a container where a runscript is defined apptainer pull <resource>://<container> Pulls a container from a container registry apptainer build --sandbox <sbox> <container> Builds a sandbox from a container apptainer build <container> <sbox> Builds a container from a sandbox","title":"Useful Apptainer Commands"},{"location":"05_UsingSoftware/#building-container-images","text":"Containers can be made from scratch using a definition file , or recipe file, which is a text file that specifies the base image, the software to be installed, and other information. The documentation for the apptainer build command shows the full usage for the build command. Container images can also be bootstrapped from other images, found on Docker Hub for instance. The recommended workflow for building containers is shown below:","title":"Building Container Images"},{"location":"05_UsingSoftware/#software-specific-guides","text":"","title":"Software-Specific Guides"},{"location":"05_UsingSoftware/#python","text":"Python is a high-level, general-purpose programming language.","title":"Python"},{"location":"05_UsingSoftware/#python-versions-on-rc","text":"Python is available by default to all users on the system software stack, and it is also available on the central software stack. Additionally, users can install their own instances of Python in a variety of ways in either their userspace or in group spaces.","title":"Python versions on RC"},{"location":"05_UsingSoftware/#install-python-packages-with-pip","text":"Python packages can be installed easily using pip . By default, pip will attempt to install packages to a system location, but RC is a shared system. Users do not have write access to system locations on shared systems. The packages can instead be installed in ~/.local , which is a user location, using the following: $ pip install --user <package> Also, packages can be installed to a custom specified location using the --target option: $ pip install --target=<install_dir> <package> Note that if pip is not available, simply try pip3 (for python3) or pip2 (for python2) instead.","title":"Install Python Packages with pip"},{"location":"05_UsingSoftware/#r","text":"R is a free software environment for statistical computing and graphics.","title":"R"},{"location":"05_UsingSoftware/#r-versions-on-rc","text":"R users should make sure that the version of R remains consistent. RC has several R versions available, and when a package is installed in one version, it is not always accessible when operating in another version. Always check the R version and remain consistent! R modules can be loaded from the central software stack, and R can also be installed by users in a variety of ways within their userspace or group spaces.","title":"R Versions on RC"},{"location":"05_UsingSoftware/#install-r-packages","text":"R manages some dependencies and versions through the CRAN-like repos. R packages can be installed from within the R console with the following command: > install.packages( <package> ) Upon running the install command, a warning usually appears stating that the default system install location is not writable, so it asks to install in a personal library instead. After entering \"yes\" as a response, it may then ask to create a personal library location. Responding \"yes\" again will proceed with the installation, probably by asking to select a CRAN repository. The default personal directory described above will install the package in the ~/R/ directory. An install location can instead be supplied to the install command using the lib argument: > install.packages( \"<package>\", lib=\"<install_location>\" ) After installation, packages can then be loaded using the following command in the R console: > library( <package> ) If the package was installed in a non-standard location, then the package can be loaded from that custom install location using the lic.loc argument of the library() command: > library( <package>, lib.loc=\"<install_location>\" ) It is recommended to review dependencies of any packages to be installed because additional software may have to be loaded in the environment before launching the R console. For example, some R packages utilize CMake to perform the installation. In that case, the cmake module should be loaded before launching the R console session.","title":"Install R Packages"},{"location":"05_UsingSoftware/#r-package-installation-example","text":"To install the ggplot2 R package, first search ggplot2 online to see if there are installation instructions. A quick search shows that ggplot2 is included in the tidyverse package and that the recommended installation instructions are the following: # The easiest ways to get ggplot2 is to install the whole tidyverse package: > install.packages(\"tidyverse\") # Alternatively, install just ggplot2: > install.packages(\"ggplot2\") Searching for install instructions usually provides all the necessary information! Some R packages may require changes to the user environment before the package can be installed successfully within the R console. Typically, the user environment change is as simple as accessing a newer compiler version by loading a software module like intel with $ module load intel Sometimes, installing R packages may be a little more involved. To install the units R package, for example, an additional library must be downloaded and installed locally in order for the package to be installed properly. To install the units R package on RC, perform the following commands in an interactive session on a compute node: $ cd ~/scratch $ wget https://downloads.unidata.ucar.edu/udunits/2.2.28/udunits-2.2.28.tar.gz $ tar -xvf udunits-2.2.28.tar.gz $ cd udunits-2.2.28 $ ./configure prefix=$HOME/.local $ make $ make install $ export UDUNITS2_INCLUDE=$HOME/.local/include $ export UDUNITS2_LIBS=$HOME/.local/lib $ export LD_LIBRARY_PATH=$HOME/.local/lib:$LD_LIBRARY_PATH $ module load r/4.2.1 $ R > install.packages(\"units\") > library(units)","title":"R Package Installation Example"},{"location":"05_UsingSoftware/#r-packages-with-anaconda","text":"The R installation itself and its R packages can be easily installed and managed within an conda environment. Creating a conda environment containing its own R installation and some R packages can be accomplished with the following: (base) $ conda create -n r_env (base) $ conda activate r_env (r_env) $ conda install r-base (r_env) $ conda install r-tidyverse Note that after r-base is the base R installation, and R packages (or in the case of tidyverse , a bundle of packages) usually are named r-* in conda repos. Alternatively, the creation of this environment can be completed with a single line. (base) $ conda create -n r_env r-base r-tidyverse","title":"R Packages with Anaconda"}]}